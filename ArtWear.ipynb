{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd5o8uGvS32C",
        "outputId": "923dd510-51d1-4db7-c76c-1502f2c11529"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m-ifinPjnw9",
        "outputId": "fcd2631c-30b5-4ecc-94d1-aafd280a180f"
      },
      "outputs": [],
      "source": [
        "# Create a directory in drive\n",
        "import os\n",
        "work_dir = '/content/drive/MyDrive/Clothes-Project'\n",
        "os.makedirs(work_dir, exist_ok=True)\n",
        "os.chdir(work_dir)\n",
        "print(f\"Working directory: {work_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QTFv7y2TcIG",
        "outputId": "c3b34156-8624-4c33-e54a-52d1268d2253"
      },
      "outputs": [],
      "source": [
        "# GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "aZtGdUYzTejH",
        "outputId": "506047db-28aa-4a6e-b132-9d2c9e0fc84e"
      },
      "outputs": [],
      "source": [
        "# Upload dataset\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "def upload_and_extract_dataset():\n",
        "    print(\"Upload your dataset ZIP file\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"Extracting {filename}...\")\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "        print(\"Dataset extracted\")\n",
        "\n",
        "upload_and_extract_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xyqjMS0kJw3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications import MobileNetV2, Xception\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict\n",
        "from datetime import datetime\n",
        "import random\n",
        "import os\n",
        "from tensorflow.nn import softmax\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny03rLjUeagJ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "dataset_dir = Path(\"/content/drive/MyDrive/Clothes-Project/clothes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKrnNNZIgTyA",
        "outputId": "175a896d-bc7b-45e8-cc73-eed19c8d97eb"
      },
      "outputs": [],
      "source": [
        "def count_images_per_class(base_dir):\n",
        "    base_dir = Path(base_dir)\n",
        "    class_counts = {}\n",
        "\n",
        "    for class_dir in sorted(base_dir.iterdir()):\n",
        "        if class_dir.is_dir():\n",
        "            image_extensions = ('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')\n",
        "            images = [f for f in class_dir.iterdir()\n",
        "                     if f.is_file() and f.suffix in image_extensions]\n",
        "            class_counts[class_dir.name] = len(images)\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "\n",
        "dataset_counts = count_images_per_class(dataset_dir)\n",
        "\n",
        "total_images = sum(dataset_counts.values())\n",
        "print(f\"\\nTotal images: {total_images}\")\n",
        "print(f\"Number of classes: {len(dataset_counts)}\")\n",
        "print(\"\\nImages per class:\")\n",
        "\n",
        "for class_name, count in sorted(dataset_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    percentage = (count / total_images) * 100\n",
        "    print(f\"  {class_name:30s}: {count:4d} images ({percentage:5.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWPmubJKgW-c",
        "outputId": "bcbed398-4572-47da-fd94-ed7135e7c56c"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "def analyze_image_properties(base_dir, sample_size=200):\n",
        "    base_dir = Path(base_dir)\n",
        "    image_paths = []\n",
        "\n",
        "    for class_dir in base_dir.iterdir():\n",
        "        if class_dir.is_dir():\n",
        "            for img_file in class_dir.iterdir():\n",
        "                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                    image_paths.append(img_file)\n",
        "\n",
        "    # Take sample before split\n",
        "    sample = random.sample(image_paths, min(sample_size, len(image_paths)))\n",
        "\n",
        "    widths, heights, formats, corrupted = [], [], [], []\n",
        "\n",
        "    for img_path in sample:\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                widths.append(img.width)\n",
        "                heights.append(img.height)\n",
        "                formats.append(img.format)\n",
        "        except:\n",
        "            corrupted.append(str(img_path))\n",
        "\n",
        "    print(f\"\\nAnalyzed {len(sample)} images\")\n",
        "\n",
        "    if widths:\n",
        "        print(\"\\nImage Dimensions:\")\n",
        "        print(f\"  Width  - Min: {min(widths):4d}, Max: {max(widths):4d}, Mean: {np.mean(widths):6.1f}\")\n",
        "        print(f\"  Height - Min: {min(heights):4d}, Max: {max(heights):4d}, Mean: {np.mean(heights):6.1f}\")\n",
        "\n",
        "        print(\"\\nFormat Distribution:\")\n",
        "        format_counts = Counter(formats)\n",
        "        for fmt, count in format_counts.most_common():\n",
        "            print(f\"  {fmt}: {count} images ({count/len(sample)*100:.1f}%)\")\n",
        "\n",
        "    if corrupted:\n",
        "        print(f\"\\nFound {len(corrupted)} corrupted images:\")\n",
        "        for path in corrupted[:5]:\n",
        "            print(f\"  - {path}\")\n",
        "    else:\n",
        "        print(\"\\nNo corrupted images found\")\n",
        "\n",
        "    return widths, heights\n",
        "\n",
        "widths, heights = analyze_image_properties(dataset_dir, sample_size=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9_2gOQ5ga1h",
        "outputId": "094c464d-8c1c-4f91-ab32-0fad178a6346"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "def split_dataset(dataset_dir, output_dir):\n",
        "    dataset_dir = Path(dataset_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "\n",
        "    # Create split folders\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        (output_dir / split).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for class_dir in dataset_dir.iterdir():\n",
        "        if class_dir.is_dir():\n",
        "            class_name = class_dir.name\n",
        "\n",
        "            # Make class folders\n",
        "            for split in [\"train\", \"val\", \"test\"]:\n",
        "                (output_dir / split / class_name).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # List images\n",
        "            images = [img for img in class_dir.iterdir()\n",
        "                      if img.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
        "            random.shuffle(images)\n",
        "\n",
        "            # Step 1: 80% train_temp, 20% test\n",
        "            n = len(images)\n",
        "            train_set1_end = int(n * 0.8)\n",
        "            train_set1 = images[:train_set1_end]\n",
        "            test_set = images[train_set1_end:]\n",
        "\n",
        "            # Step 2: 80% of train_temp = train_final\n",
        "            train_set_end = int(len(train_set1) * 0.8)\n",
        "            train_set = train_set1[:train_set_end]\n",
        "            val_set = train_set1[train_set_end:]\n",
        "\n",
        "            # Copy files\n",
        "            for img in train_set:\n",
        "                shutil.copy(img, output_dir / \"train\" / class_name)\n",
        "            for img in val_set:\n",
        "                shutil.copy(img, output_dir / \"val\" / class_name)\n",
        "            for img in test_set:\n",
        "                shutil.copy(img, output_dir / \"test\" / class_name)\n",
        "\n",
        "            print(f\"{class_name}: {len(train_set)} train, {len(val_set)} val, {len(test_set)} test\")\n",
        "\n",
        "dataset_dir = \"/content/drive/MyDrive/Clothes-Project/clothes\"\n",
        "output_dir  = \"/content/drive/MyDrive/Clothes-Project/clothes_split\"\n",
        "\n",
        "split_dataset(dataset_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3GTisSsksOg",
        "outputId": "6e07c11e-f8fc-4a83-b954-7209cf985df0"
      },
      "outputs": [],
      "source": [
        "class ClothingClassifier:\n",
        "\n",
        "    CATEGORIES = {\n",
        "        0: 'glasses', 1: 'hat', 2: 'outwear', 3: 'pants', 4: 'shirt',\n",
        "        5: 'shoes', 6: 'skirt', 7: 't-shirt', 8: 'watches'\n",
        "    }\n",
        "\n",
        "    CATEGORY_MAPPING = {\n",
        "        'glasses': 'accessory', 'hat': 'accessory', 'outwear': 'outer',\n",
        "        'pants': 'bottom', 'shirt': 'top', 'shoes': 'shoes',\n",
        "        'skirt': 'bottom', 't-shirt': 'top', 'watches': 'accessory'\n",
        "    }\n",
        "\n",
        "    def __init__(self, model_path=None):\n",
        "        self.model = None\n",
        "        self.image_size = (150, 150)\n",
        "        self.num_classes = 9\n",
        "\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            self.load_model(model_path)\n",
        "        else:\n",
        "            self.model = self.build_model()\n",
        "\n",
        "    def build_model(self, learning_rate=0.001, dropout_rate=0.4):\n",
        "        # Build MobileNetV2-based classifier\n",
        "        base_model = MobileNetV2(\n",
        "            weights='imagenet',\n",
        "            input_shape=(self.image_size[0], self.image_size[1], 3),\n",
        "            include_top=False\n",
        "        )\n",
        "        base_model.trainable = False\n",
        "\n",
        "        inputs = keras.Input(shape=(self.image_size[0], self.image_size[1], 3))\n",
        "        x = base_model(inputs, training=False)\n",
        "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "        x = keras.layers.Dense(128, activation='relu')(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "        outputs = keras.layers.Dense(self.num_classes)(x)\n",
        "\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate),\n",
        "            loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def train(self, train_dir, val_dir, epochs=30, save_path='best_model.h5'):\n",
        "        # Train with automatic history saving\n",
        "        train_gen = ImageDataGenerator(\n",
        "            preprocessing_function=preprocess_input,\n",
        "            rotation_range=30, width_shift_range=0.2, height_shift_range=0.2,\n",
        "            shear_range=0.1, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest'\n",
        "        )\n",
        "        train_ds = train_gen.flow_from_directory(\n",
        "            train_dir, target_size=self.image_size, batch_size=32, seed=1, shuffle=True\n",
        "        )\n",
        "\n",
        "        val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "        val_ds = val_gen.flow_from_directory(\n",
        "            val_dir, target_size=self.image_size, batch_size=32, seed=1, shuffle=False\n",
        "        )\n",
        "\n",
        "        print(f\"\\nFound {train_ds.samples} training images\")\n",
        "        print(f\"Found {val_ds.samples} validation images\")\n",
        "        print(f\"Classes: {list(train_ds.class_indices.keys())}\\n\")\n",
        "\n",
        "        checkpoint_path = save_path.replace('.h5', '_checkpoint.h5')\n",
        "        history_path = save_path.replace('.h5', '_history.pkl')\n",
        "\n",
        "        callbacks = [\n",
        "            keras.callbacks.ModelCheckpoint(save_path, monitor=\"val_accuracy\", save_best_only=True, mode='max', verbose=1),\n",
        "            keras.callbacks.ModelCheckpoint(checkpoint_path, monitor=\"val_accuracy\", save_best_only=False, save_freq='epoch', verbose=0),\n",
        "            keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1),\n",
        "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)\n",
        "\n",
        "        # Save history\n",
        "        try:\n",
        "            with open(history_path, 'wb') as f:\n",
        "                pickle.dump(history.history, f)\n",
        "            print(f\"\\nTraining history saved to {history_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nWarning: Could not save history: {e}\")\n",
        "\n",
        "        # Cleanup checkpoint\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            try:\n",
        "                os.remove(checkpoint_path)\n",
        "                print(\"Checkpoint cleaned up\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return history\n",
        "\n",
        "    def load_history(self, save_path='best_model.h5'):\n",
        "        # Load saved training history\n",
        "        history_path = save_path.replace('.h5', '_history.pkl')\n",
        "        if os.path.exists(history_path):\n",
        "            with open(history_path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        return None\n",
        "\n",
        "    def evaluate(self, test_dir):\n",
        "        # Evaluate model on test set\n",
        "        test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "        test_ds = test_gen.flow_from_directory(test_dir, target_size=self.image_size, batch_size=32, shuffle=False)\n",
        "        print(f\"\\nEvaluating on {test_ds.samples} test images...\")\n",
        "        results = self.model.evaluate(test_ds, verbose=1)\n",
        "        print(f\"\\nTest Loss: {results[0]:.4f}\")\n",
        "        print(f\"Test Accuracy: {results[1]:.4f}\")\n",
        "        return results\n",
        "\n",
        "    def predict(self, image_path):\n",
        "        # Predict single image\n",
        "        img = load_img(image_path, target_size=self.image_size)\n",
        "        x = img_to_array(img)\n",
        "        X = preprocess_input(np.array([x]))\n",
        "        pred = self.model.predict(X, verbose=0)\n",
        "        pred_probs = tf.nn.softmax(pred[0]).numpy()\n",
        "        top_idx = np.argmax(pred_probs)\n",
        "\n",
        "        return {\n",
        "            'category': self.CATEGORIES[top_idx],\n",
        "            'simplified_category': self.CATEGORY_MAPPING[self.CATEGORIES[top_idx]],\n",
        "            'confidence': float(pred_probs[top_idx]),\n",
        "            'all_probabilities': {self.CATEGORIES[i]: float(pred_probs[i]) for i in range(len(pred_probs))}\n",
        "        }\n",
        "\n",
        "    def predict_batch(self, test_dir):\n",
        "        # Batch predictions for confusion matrix\n",
        "        test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "        test_ds = test_gen.flow_from_directory(test_dir, target_size=self.image_size, batch_size=32, shuffle=False)\n",
        "        predictions = self.model.predict(test_ds, verbose=1)\n",
        "        pred_classes = np.argmax(predictions, axis=1)\n",
        "        return pred_classes, test_ds.classes, list(test_ds.class_indices.keys())\n",
        "\n",
        "    def save_model(self, path='clothing_classifier.h5'):\n",
        "        self.model.save(path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model = keras.models.load_model(path)\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "print(\"ClothingClassifier class loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8EE4o2zk0rJ",
        "outputId": "0769bc0b-47e1-4999-8c65-e1b47c8d7f4b"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/Clothes-Project\"\n",
        "\n",
        "train_dir = f\"{BASE_DIR}/clothes_split/train\"\n",
        "val_dir = f\"{BASE_DIR}/clothes_split/val\"\n",
        "test_dir = f\"{BASE_DIR}/clothes_split/test\"\n",
        "model_save_path = f\"{BASE_DIR}/best_mobile_model.h5\"\n",
        "\n",
        "print(f\"Train directory: {train_dir}\")\n",
        "print(f\"Val directory: {val_dir}\")\n",
        "print(f\"Test directory: {test_dir}\")\n",
        "print(f\"Model save path: {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8Q_qvYKlFhP",
        "outputId": "801c293d-a71b-49c0-f537-3ccd4721327a"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Check if model already exists\n",
        "if os.path.exists(model_save_path):\n",
        "    print(f\"Found existing model at {model_save_path}\")\n",
        "    print(\"Loading existing model...\")\n",
        "    classifier = ClothingClassifier(model_path=model_save_path)\n",
        "    TRAINING_COMPLETED = False\n",
        "else:\n",
        "    print(\"No existing model found. Starting training...\")\n",
        "    classifier = ClothingClassifier(model_path=None)\n",
        "\n",
        "    print(\"TRAINING CLASSIFIER\")\n",
        "    history = classifier.train(\n",
        "        train_dir=train_dir,\n",
        "        val_dir=val_dir,\n",
        "        epochs=30,\n",
        "        save_path=model_save_path\n",
        "    )\n",
        "\n",
        "    best_val_acc = max(history.history['val_accuracy'])\n",
        "    print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.4f}\")\n",
        "    TRAINING_COMPLETED = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "biUy3eZjlKK5",
        "outputId": "3270f4b7-1031-4568-cef3-fa0f51378338"
      },
      "outputs": [],
      "source": [
        "history_dict = classifier.load_history(model_save_path)\n",
        "\n",
        "if history_dict:\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_dict['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "    plt.plot(history_dict['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.title('Model Accuracy', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_dict['loss'], label='Train Loss', linewidth=2)\n",
        "    plt.plot(history_dict['val_loss'], label='Val Loss', linewidth=2)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.title('Model Loss', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{BASE_DIR}/training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Training plots saved\")\n",
        "else:\n",
        "    print(\"No training history found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBzIdVH3lO1V",
        "outputId": "616056e4-8cc8-461f-d17f-0767f13b59b7"
      },
      "outputs": [],
      "source": [
        "test_results = classifier.evaluate(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-VUxgdlsldu7",
        "outputId": "01983007-1661-48de-ca47-953192809e3e"
      },
      "outputs": [],
      "source": [
        "print(\"DETAILED PERFORMANCE METRICS\")\n",
        "pred_classes, true_classes, class_labels = classifier.predict_batch(test_dir)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_classes, pred_classes, target_names=class_labels))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(true_classes, pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_labels, yticklabels=class_labels,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix', fontsize=16)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{BASE_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion matrix saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12p_4SMqRJJu",
        "outputId": "fa6f9ada-93df-4981-a21c-a97393893b70"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Clothes-Project/best_mobile_model.h5\"\n",
        "classifier = load_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FkQ_k6nHzGeO",
        "outputId": "c5fd7e54-4260-46c9-9599-a60bf1c17488"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(test_dir, num_samples=9):\n",
        "    CLASS_NAMES = ['glasses', 'hat', 'outwear', 'pants', 'shirt', 'shoes', 'skirt', 't-shirt', 'watches']\n",
        "\n",
        "    test_path = Path(test_dir)\n",
        "    sample_images = []\n",
        "\n",
        "    for class_dir in test_path.iterdir():\n",
        "        if class_dir.is_dir():\n",
        "            images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.jpeg')) + list(class_dir.glob('*.png'))\n",
        "            if images:\n",
        "                sample_images.append(random.choice(images))\n",
        "\n",
        "    sample_images = random.sample(sample_images, min(num_samples, len(sample_images)))\n",
        "\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, img_path in enumerate(sample_images):\n",
        "        img = load_img(img_path, target_size=(150, 150))\n",
        "        img_array = img_to_array(img) / 255.0\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        # Get raw predictions\n",
        "        raw_preds = classifier.predict(img_array)[0]\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        preds = softmax(raw_preds).numpy()\n",
        "\n",
        "        pred_label = CLASS_NAMES[np.argmax(preds)]\n",
        "        confidence = np.max(preds)\n",
        "\n",
        "        true_label = img_path.parent.name\n",
        "        color = 'green' if true_label == pred_label else 'red'\n",
        "\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].axis('off')\n",
        "        axes[idx].set_title(\n",
        "            f\"True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2%}\",\n",
        "            fontsize=10, color=color, weight='bold'\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/sample_predictions.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(test_dir, num_samples=9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2Vjng7eNNHM",
        "outputId": "70408fd9-5a1b-4886-a9fb-92a5c683dc11"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class WardrobeItem:\n",
        "    # Wardrobe items class\n",
        "    id: str\n",
        "    image_path: str\n",
        "    category: str\n",
        "    simplified_category: str\n",
        "    confidence: float\n",
        "    embedding: np.ndarray = None\n",
        "    added_date: str = None\n",
        "\n",
        "    def to_dict(self):\n",
        "        data = asdict(self)\n",
        "        if self.embedding is not None:\n",
        "            data['embedding'] = self.embedding.tolist()\n",
        "        return data\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data):\n",
        "        if data.get('embedding') is not None:\n",
        "            data['embedding'] = np.array(data['embedding'])\n",
        "        return cls(**data)\n",
        "\n",
        "\n",
        "class OutfitGenerator:\n",
        "    # Generate outfit recommendations using visual similarity\n",
        "\n",
        "    def __init__(self):\n",
        "        self.embedding_model = self._build_embedding_model()\n",
        "        self.image_size = (150, 150)\n",
        "\n",
        "    def _build_embedding_model(self):\n",
        "        # Build embedding model for visual similarity\n",
        "        base_model = MobileNetV2(\n",
        "            weights='imagenet',\n",
        "            input_shape=(150, 150, 3),\n",
        "            include_top=False\n",
        "        )\n",
        "        base_model.trainable = False\n",
        "\n",
        "        inputs = keras.Input(shape=(150, 150, 3))\n",
        "        base = base_model(inputs, training=False)\n",
        "        vector = keras.layers.GlobalAveragePooling2D()(base)\n",
        "\n",
        "        model = keras.Model(inputs, vector)\n",
        "        return model\n",
        "\n",
        "    def get_embedding(self, image_path):\n",
        "        # Get normalized embedding for an image\n",
        "        img = load_img(image_path, target_size=self.image_size)\n",
        "        x = img_to_array(img)\n",
        "        X = preprocess_input(np.array([x]))\n",
        "        embedding = self.embedding_model.predict(X, verbose=0)[0]\n",
        "        embedding = embedding / (np.linalg.norm(embedding) + 1e-8)\n",
        "        return embedding\n",
        "\n",
        "    def compute_embeddings(self, wardrobe: List[WardrobeItem]):\n",
        "        # Compute embeddings for all items\n",
        "        for item in wardrobe:\n",
        "            if item.embedding is None:\n",
        "                item.embedding = self.get_embedding(item.image_path)\n",
        "        return wardrobe\n",
        "\n",
        "    def visual_similarity(self, item1: WardrobeItem, item2: WardrobeItem) -> float:\n",
        "        # Calculate visual similarity between two items\n",
        "        if item1.embedding is None or item2.embedding is None:\n",
        "            return 0.0\n",
        "        similarity = np.dot(item1.embedding, item2.embedding)\n",
        "        return (similarity + 1) / 2\n",
        "\n",
        "    def compatibility_score(self, item1: WardrobeItem, item2: WardrobeItem) -> float:\n",
        "        # Calculate compatibility score between two items\n",
        "        base_score = self.visual_similarity(item1, item2)\n",
        "        cat1 = item1.simplified_category\n",
        "        cat2 = item2.simplified_category\n",
        "\n",
        "        if cat1 == cat2:\n",
        "            return base_score * 0.2\n",
        "\n",
        "        compatible_pairs = {\n",
        "            ('top', 'bottom'): 1.3, ('bottom', 'shoes'): 1.2,\n",
        "            ('top', 'outer'): 1.15, ('bottom', 'outer'): 1.1,\n",
        "            ('top', 'shoes'): 1.1\n",
        "        }\n",
        "\n",
        "        pair = tuple(sorted([cat1, cat2]))\n",
        "        if pair in compatible_pairs:\n",
        "            base_score *= compatible_pairs[pair]\n",
        "\n",
        "        return min(base_score, 1.0)\n",
        "\n",
        "    def calculate_outfit_score(self, items: List[WardrobeItem]) -> float:\n",
        "        #Calculate overall outfit score\n",
        "        if len(items) < 2:\n",
        "            return 0.0\n",
        "\n",
        "        scores = []\n",
        "        for i in range(len(items)):\n",
        "            for j in range(i + 1, len(items)):\n",
        "                score = self.compatibility_score(items[i], items[j])\n",
        "                scores.append(score)\n",
        "\n",
        "        return np.mean(scores)\n",
        "\n",
        "    def generate_outfit(self, wardrobe: List[WardrobeItem],\n",
        "                       anchor_item: WardrobeItem = None,\n",
        "                       min_score: float = 0.5, attempts: int = 100,\n",
        "                       include_accessories: bool = True) -> Dict:\n",
        "        wardrobe = self.compute_embeddings(wardrobe)\n",
        "\n",
        "        # Separate items by category\n",
        "        tops = [i for i in wardrobe if i.simplified_category == 'top']\n",
        "        bottoms = [i for i in wardrobe if i.simplified_category == 'bottom']\n",
        "        shoes = [i for i in wardrobe if i.simplified_category == 'shoes']\n",
        "        outers = [i for i in wardrobe if i.simplified_category == 'outer']\n",
        "        accessories = [i for i in wardrobe if i.simplified_category == 'accessory']\n",
        "\n",
        "        # Further separate accessories into glasses and hats\n",
        "        glasses = [i for i in accessories if i.category == 'glasses']\n",
        "        hats = [i for i in accessories if i.category == 'hat']\n",
        "        watches = [i for i in accessories if i.category == 'watches']\n",
        "\n",
        "        if not tops or not bottoms or not shoes:\n",
        "            return {'success': False, 'message': 'Need at least one top, bottom, and shoes'}\n",
        "\n",
        "        best_outfit = None\n",
        "        best_score = 0.0\n",
        "\n",
        "        for _ in range(attempts):\n",
        "            # Core outfit: top, bottom, shoes\n",
        "            top = np.random.choice(tops)\n",
        "            bottom = np.random.choice(bottoms)\n",
        "            shoe = np.random.choice(shoes)\n",
        "            outfit = [top, bottom, shoe]\n",
        "\n",
        "            # Add outer layer (30% chance)\n",
        "            if outers and np.random.random() < 0.3:\n",
        "                outfit.append(np.random.choice(outers))\n",
        "\n",
        "            # Add accessories\n",
        "            if include_accessories:\n",
        "                # Add glasses if available\n",
        "                if glasses:\n",
        "                    outfit.append(np.random.choice(glasses))\n",
        "\n",
        "                # Add hat if available\n",
        "                if hats:\n",
        "                    outfit.append(np.random.choice(hats))\n",
        "\n",
        "                # Add watch if available\n",
        "                if watches:\n",
        "                    outfit.append(np.random.choice(watches))\n",
        "\n",
        "            score = self.calculate_outfit_score(outfit)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_outfit = outfit\n",
        "\n",
        "        if best_score < min_score:\n",
        "            return {'success': False, 'message': f'Best score {best_score:.2f} below threshold'}\n",
        "\n",
        "        # Categorize outfit items for display\n",
        "        outfit_breakdown = {\n",
        "            'top': [i for i in best_outfit if i.simplified_category == 'top'],\n",
        "            'bottom': [i for i in best_outfit if i.simplified_category == 'bottom'],\n",
        "            'shoes': [i for i in best_outfit if i.simplified_category == 'shoes'],\n",
        "            'outer': [i for i in best_outfit if i.simplified_category == 'outer'],\n",
        "            'accessories': [i for i in best_outfit if i.simplified_category == 'accessory']\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'items': best_outfit,\n",
        "            'score': best_score,\n",
        "            'type': 'complete_outfit',\n",
        "            'breakdown': outfit_breakdown\n",
        "        }\n",
        "\n",
        "print(\"Outfit Generator classes loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T126mkpoNwYo",
        "outputId": "bc7ba1c0-29cb-457f-d4e3-f75a7d1ef5f6"
      },
      "outputs": [],
      "source": [
        "class WardrobeManager:\n",
        "    # Manage wardrobe items and generate outfit recommendations\n",
        "    def __init__(self, classifier: ClothingClassifier,\n",
        "                 outfit_generator: AIOutfitGenerator,\n",
        "                 storage_path='wardrobe_data.json'):\n",
        "        self.classifier = classifier\n",
        "        self.outfit_generator = outfit_generator\n",
        "        self.storage_path = storage_path\n",
        "        self.wardrobe: List[WardrobeItem] = []\n",
        "        self.load_wardrobe()\n",
        "\n",
        "    def add_item(self, image_path: str) -> WardrobeItem:\n",
        "        # Add item to wardrobe\n",
        "        classification = self.classifier.predict(image_path)\n",
        "\n",
        "        item_id = f\"item_{len(self.wardrobe)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        item = WardrobeItem(\n",
        "            id=item_id,\n",
        "            image_path=image_path,\n",
        "            category=classification['category'],\n",
        "            simplified_category=classification['simplified_category'],\n",
        "            confidence=classification['confidence'],\n",
        "            added_date=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "        item.embedding = self.outfit_generator.get_embedding(image_path)\n",
        "        self.wardrobe.append(item)\n",
        "        self.save_wardrobe()\n",
        "\n",
        "        print(f\" Added: {classification['category']} (confidence: {classification['confidence']:.2%})\")\n",
        "        return item\n",
        "\n",
        "    def remove_item(self, item_id: str):\n",
        "        # Remove item from wardrobe\n",
        "        self.wardrobe = [item for item in self.wardrobe if item.id != item_id]\n",
        "        self.save_wardrobe()\n",
        "        print(f\"Item {item_id} removed\")\n",
        "\n",
        "    def get_outfit_recommendation(self, anchor_item_id: str = None,\n",
        "                                 include_accessories: bool = True) -> Dict:\n",
        "        # Get outfit recommendation\n",
        "        anchor = None\n",
        "        if anchor_item_id:\n",
        "            anchor = next((item for item in self.wardrobe if item.id == anchor_item_id), None)\n",
        "        return self.outfit_generator.generate_outfit(\n",
        "            self.wardrobe,\n",
        "            anchor_item=anchor,\n",
        "            include_accessories=include_accessories\n",
        "        )\n",
        "\n",
        "    def save_wardrobe(self):\n",
        "        # Save wardrobe to JSON\n",
        "        data = {\n",
        "            'items': [item.to_dict() for item in self.wardrobe],\n",
        "            'last_updated': datetime.now().isoformat()\n",
        "        }\n",
        "        with open(self.storage_path, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "\n",
        "    def load_wardrobe(self):\n",
        "        # Load wardrobe from JSON\n",
        "        if os.path.exists(self.storage_path):\n",
        "            with open(self.storage_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                self.wardrobe = [WardrobeItem.from_dict(item) for item in data['items']]\n",
        "            print(f\"Loaded {len(self.wardrobe)} items from wardrobe\")\n",
        "\n",
        "    def list_items(self):\n",
        "        # List all wardrobe items\n",
        "        print(f\"WARDROBE INVENTORY ({len(self.wardrobe)} items)\")\n",
        "        for item in self.wardrobe:\n",
        "            print(f\"{item.id}: {item.category} ({item.simplified_category}) - {item.confidence:.2%}\")\n",
        "\n",
        "print(\"Wardrobe Manager class loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLwv9ko3RBrX",
        "outputId": "6b79c563-1453-4ee9-fdb6-d3d3d7b12c69"
      },
      "outputs": [],
      "source": [
        "print(\"INITIALIZING CLOTHES SYSTEM\")\n",
        "\n",
        "# Initialize outfit generator\n",
        "outfit_generator = OutfitGenerator()\n",
        "\n",
        "# Initialize the ClothingClassifier\n",
        "model_path = \"/content/drive/MyDrive/Clothes-Project/best_mobile_model.h5\"\n",
        "classifier = ClothingClassifier(model_path=model_path)\n",
        "\n",
        "# Initialize wardrobe manager\n",
        "wardrobe_storage_path = f\"{BASE_DIR}/my_wardrobe.json\"\n",
        "manager = WardrobeManager(\n",
        "    classifier=classifier,\n",
        "    outfit_generator=outfit_generator,\n",
        "    storage_path=wardrobe_storage_path\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nFashion System ready!\")\n",
        "print(f\"Wardrobe storage: {wardrobe_storage_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RgngXDpTR6nL",
        "outputId": "27a816fb-95e0-4f82-f7d0-9d5d4acfc1ff"
      },
      "outputs": [],
      "source": [
        "def add_clothing_items():\n",
        "    # Upload images and add to wardrobe\n",
        "    from google.colab import files\n",
        "    from IPython.display import Image, display\n",
        "\n",
        "    print(\"Upload clothing images...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename, data in uploaded.items():\n",
        "        # Save uploaded file\n",
        "        filepath = f\"{BASE_DIR}/uploads/{filename}\"\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(data)\n",
        "\n",
        "        # Display image\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing: {filename}\")\n",
        "        display(Image(filepath, width=200))\n",
        "\n",
        "        # Add to wardrobe\n",
        "        item = manager.add_item(filepath)\n",
        "        print(f\"Category: {item.category}\")\n",
        "        print(f\"Confidence: {item.confidence:.2%}\")\n",
        "\n",
        "add_clothing_items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYvjzABOVr1S"
      },
      "outputs": [],
      "source": [
        "def predict_image(image_path, classifier):\n",
        "    CLASS_NAMES = ['glasses', 'hat', 'outwear', 'pants', 'shirt', 'shoes', 'skirt', 't-shirt', 'watches']\n",
        "\n",
        "    # Load and preprocess\n",
        "    img = load_img(image_path, target_size=(150, 150))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    preds = classifier.predict(img_array)[0]\n",
        "\n",
        "    class_id = np.argmax(preds)\n",
        "    confidence = preds[class_id]\n",
        "\n",
        "    return CLASS_NAMES[class_id], confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5TG-NpVtSD_r",
        "outputId": "66b95a77-630d-40a1-ac82-f888da228da0"
      },
      "outputs": [],
      "source": [
        "def show_outfit_recommendation(include_accessories=True):\n",
        "    # Generate and display outfit recommendation\n",
        "    from IPython.display import Image, display\n",
        "\n",
        "    print(\"GENERATING OUTFIT RECOMMENDATION\")\n",
        "\n",
        "    result = manager.get_outfit_recommendation(include_accessories=include_accessories)\n",
        "\n",
        "    if result['success']:\n",
        "        print(f\"\\nâœ“ Outfit Score: {result['score']:.2f}/1.00\")\n",
        "        print(f\"Type: {result['type']}\")\n",
        "\n",
        "        # Display breakdown by category\n",
        "        if 'breakdown' in result:\n",
        "            breakdown = result['breakdown']\n",
        "\n",
        "            print(\"YOUR COMPLETE OUTFIT\")\n",
        "\n",
        "            # Display by category\n",
        "            categories_display = [\n",
        "                ('TOP', 'top'),\n",
        "                ('BOTTOM', 'bottom'),\n",
        "                ('SHOES', 'shoes'),\n",
        "                ('OUTER LAYER', 'outer'),\n",
        "                ('ACCESSORIES', 'accessories')\n",
        "            ]\n",
        "\n",
        "            item_count = 1\n",
        "            for display_name, cat_key in categories_display:\n",
        "                items = breakdown.get(cat_key, [])\n",
        "                if items:\n",
        "                    print(f\"\\n{display_name}:\")\n",
        "                    for item in items:\n",
        "                        print(f\"   {item_count}. {item.category.upper()}\")\n",
        "                        print(f\"      Confidence: {item.confidence:.2%}\")\n",
        "                        print(f\"      Added: {item.added_date}\")\n",
        "\n",
        "                        # Display image\n",
        "                        if os.path.exists(item.image_path):\n",
        "                            img_path = Path(item.image_path).resolve()\n",
        "                            display(Image(str(img_path), width=200))\n",
        "                        item_count += 1\n",
        "        else:\n",
        "            # Fallback to simple display\n",
        "            print(\"\\nYour Outfit:\")\n",
        "            for i, item in enumerate(result['items'], 1):\n",
        "                print(f\"\\n{i}. {item.category.upper()} ({item.simplified_category})\")\n",
        "                print(f\"   Confidence: {item.confidence:.2%}\")\n",
        "                print(f\"   Added: {item.added_date}\")\n",
        "\n",
        "                if os.path.exists(item.image_path):\n",
        "                    display(Image(item.image_path, width=200))\n",
        "    else:\n",
        "        print(f\"{result['message']}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "show_outfit_recommendation(include_accessories=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rwz0zy5ed6B",
        "outputId": "be537058-8e44-4980-8619-ba684907b044"
      },
      "outputs": [],
      "source": [
        "manager.list_items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "21a744b3b0704789aff1994bdf0d8016",
            "3044cc5d58494879bab4b0f8fcae0160",
            "70406554e3774af8bf4ede6d901f1a77",
            "fe754f2dbc3748959bf7a193c1daaac2",
            "cd195466105d4aadb01b2ae295633dc8",
            "8819b336cdef4ddab3856436ea44f098",
            "d50973b30f774f528b01dda0b044b87a",
            "17c5fe7e2974409bbf25c9588a3afc04",
            "ca260a78ac53495c858e6be513a1eaae",
            "7130b3189cc34328bfaa765bbdcf348c",
            "9fd2fc7e1bba4a3b8ea430711bc642ff",
            "5adfaa5f70e04a198dbe4b0d5ec3c97f",
            "abd1c93b427a4ae9b7a62476f4293b83",
            "2c2e6ba9cb814ae3b02bba6f987ebbcd",
            "9da0e961006240d9bed61d805119b86a",
            "37a64ffe0700449a82e243484c14c553",
            "492d2ba4ead34962a5a770961267f78f",
            "4d238c06169b4284afb5685292e3fb84",
            "03f251a700e14461a1095baeac08a33f",
            "dea284692b114b0090d918d67ae22e4d",
            "7576eb38e65848099961744821dc7415",
            "e5a6ef01a3ba4871be4cd23cbcb0e6b1"
          ]
        },
        "id": "z15lnoBUxZt4",
        "outputId": "7f61b4f6-32da-4feb-ea39-18386aa19a64"
      },
      "outputs": [],
      "source": [
        "# Generates the outfits picture\n",
        "\n",
        "!pip install -q diffusers transformers accelerate pillow torch rembg\n",
        "!pip install -q git+https://github.com/danielgatis/rembg.git\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "from rembg import remove\n",
        "import random\n",
        "\n",
        "print(\"Loading models...\")\n",
        "\n",
        "# Load Stable Diffusion for background generation\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(\"cuda\")\n",
        "\n",
        "print(\"Models loaded!\\n\")\n",
        "\n",
        "\n",
        "def extract_outfit_images(result):\n",
        "    # Extract image paths from outfit recommendation\n",
        "    outfit_images = {}\n",
        "\n",
        "    if result.get('success') and 'breakdown' in result:\n",
        "        breakdown = result['breakdown']\n",
        "\n",
        "        # Main clothing items\n",
        "        for category in ['top', 'bottom', 'shoes']:\n",
        "            items = breakdown.get(category, [])\n",
        "            if items and len(items) > 0:\n",
        "                img_path = Path(items[0].image_path).resolve()\n",
        "                if img_path.exists():\n",
        "                    outfit_images[category] = str(img_path)\n",
        "                    print(f\"Found {category}: {img_path.name}\")\n",
        "\n",
        "        # Extract  accessories individually\n",
        "        accessories = breakdown.get('accessories', [])\n",
        "        for item in accessories:\n",
        "            img_path = Path(item.image_path).resolve()\n",
        "            if img_path.exists():\n",
        "                # Identify accessory type from category name\n",
        "                category_lower = item.category.lower()\n",
        "\n",
        "                if 'watch' in category_lower:\n",
        "                    outfit_images['watch'] = str(img_path)\n",
        "                    print(f\"Found watch: {img_path.name}\")\n",
        "                elif 'hat' in category_lower or 'cap' in category_lower or 'beanie' in category_lower:\n",
        "                    outfit_images['hat'] = str(img_path)\n",
        "                    print(f\"Found hat: {img_path.name}\")\n",
        "                elif 'glass' in category_lower or 'sunglass' in category_lower:\n",
        "                    outfit_images['glasses'] = str(img_path)\n",
        "                    print(f\"Found glasses: {img_path.name}\")\n",
        "                else:\n",
        "                    # Generic accessory\n",
        "                    if 'accessory1' not in outfit_images:\n",
        "                        outfit_images['accessory1'] = str(img_path)\n",
        "                        print(f\"Found accessory: {img_path.name}\")\n",
        "                    elif 'accessory2' not in outfit_images:\n",
        "                        outfit_images['accessory2'] = str(img_path)\n",
        "                        print(f\"Found accessory 2: {img_path.name}\")\n",
        "\n",
        "    return outfit_images\n",
        "\n",
        "# Generate ai background \n",
        "def generate_background():\n",
        "    # Generate very light, simple background\n",
        "\n",
        "    print(\"\\n Generating simple light background with AI accessories...\")\n",
        "\n",
        "    prompt = (\n",
        "        \"minimal clean white background, extremely light and bright, \"\n",
        "        \"small luxury perfume bottle in corner, tiny elegant accessories, \"\n",
        "        \"pure white surface, barely visible subtle texture, \"\n",
        "        \"very bright studio lighting, ultra clean, airy, \"\n",
        "        \"almost pure white, high key photography, overexposed bright, \"\n",
        "        \"minimalist luxury, simple elegant\"\n",
        "    )\n",
        "\n",
        "    negative_prompt = (\n",
        "        \"clothing, shirts, pants, shoes, person, model, cluttered, busy, crowded, \"\n",
        "        \"text, watermark, low quality, dark, harsh lighting, heavy shadows, grey, dim, \"\n",
        "        \"messy, chaotic, complex, detailed background, colorful, patterns\"\n",
        "    )\n",
        "\n",
        "    background = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=7.0,\n",
        "        height=768,\n",
        "        width=640\n",
        "    ).images[0]\n",
        "\n",
        "    enhancer = ImageEnhance.Brightness(background)\n",
        "    background = enhancer.enhance(1.7)\n",
        "\n",
        "    enhancer = ImageEnhance.Contrast(background)\n",
        "    background = enhancer.enhance(0.3)\n",
        "\n",
        "    background = background.filter(ImageFilter.GaussianBlur(radius=0.5))\n",
        "\n",
        "    return background\n",
        "\n",
        "# Remove items' backgrounds\n",
        "def remove_background(image_path):\n",
        "    print(f\" Removing background from {Path(image_path).name}...\")\n",
        "\n",
        "    input_image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Use rembg AI background removal\n",
        "    output_image = remove(input_image)\n",
        "\n",
        "    return output_image\n",
        "\n",
        "# Add shadows\n",
        "def create_shadow(image, offset=(10, 10), blur=15, opacity=0.3):\n",
        "    shadow = Image.new('RGBA', image.size, (0, 0, 0, 0))\n",
        "\n",
        "    if image.mode == 'RGBA':\n",
        "        alpha = image.split()[3]\n",
        "    else:\n",
        "        alpha = Image.new('L', image.size, 255)\n",
        "\n",
        "    shadow_alpha = alpha.point(lambda p: int(p * opacity))\n",
        "    shadow.putalpha(shadow_alpha)\n",
        "\n",
        "    shadow = shadow.filter(ImageFilter.GaussianBlur(blur))\n",
        "\n",
        "    return shadow\n",
        "\n",
        "\n",
        "# Composite final picture\n",
        "def create_hybrid_flatlay(outfit_images, background):\n",
        "\n",
        "    print(\"\\nCreating flatlay composition with real images...\")\n",
        "\n",
        "    canvas = background.convert('RGBA')\n",
        "    canvas_width, canvas_height = canvas.size\n",
        "\n",
        "    # Shift everything to the left to prevent right-side cropping\n",
        "    center_x = canvas_width // 2 - 50\n",
        "\n",
        "    # Main outfit centered, accessories positioned to avoid cropping\n",
        "    positions = {\n",
        "        'top': {\n",
        "            'pos': (center_x - 200, 20),\n",
        "            'rotation': 0,\n",
        "            'size': 400\n",
        "        },\n",
        "        'bottom': {\n",
        "            'pos': (center_x - 205, 290),\n",
        "            'rotation': 0,\n",
        "            'size': 410\n",
        "        },\n",
        "        'shoes': {\n",
        "            'pos': (center_x - 125, 530),\n",
        "            'rotation': -3,\n",
        "            'size': 250\n",
        "        },\n",
        "\n",
        "        'hat': {\n",
        "            'pos': (center_x + 160, 50),\n",
        "            'rotation': 8,\n",
        "            'size': 140\n",
        "        },\n",
        "        'glasses': {\n",
        "            'pos': (center_x + 165, 180),\n",
        "            'rotation': -5,\n",
        "            'size': 120\n",
        "        },\n",
        "        'watch': {\n",
        "            'pos': (center_x + 160, 290),\n",
        "            'rotation': 10,\n",
        "            'size': 115\n",
        "        },\n",
        "\n",
        "    }\n",
        "\n",
        "    # Process items in order\n",
        "    order = ['top', 'bottom', 'hat', 'glasses', 'watch', 'shoes']\n",
        "\n",
        "    for category in order:\n",
        "        if category not in outfit_images:\n",
        "            continue\n",
        "\n",
        "        config = positions.get(category)\n",
        "        if not config:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(f\"\\n Processing {category}...\")\n",
        "\n",
        "            # Remove background\n",
        "            item_img = remove_background(outfit_images[category])\n",
        "\n",
        "            # Resize\n",
        "            item_img.thumbnail((config['size'], config['size']), Image.Resampling.LANCZOS)\n",
        "\n",
        "            # Rotate\n",
        "            if category in ['top', 'bottom']:\n",
        "                angle = config['rotation']\n",
        "            else:\n",
        "                angle = config['rotation'] + random.randint(-2, 2)\n",
        "\n",
        "            item_img = item_img.rotate(angle, expand=True, fillcolor=(0,0,0,0))\n",
        "\n",
        "            # Create shadow\n",
        "            shadow = create_shadow(item_img, offset=(3, 3), blur=6, opacity=0.15)\n",
        "\n",
        "            # Position\n",
        "            x, y = config['pos']\n",
        "            if category not in ['top', 'bottom']:\n",
        "                x += random.randint(-3, 3)\n",
        "                y += random.randint(-3, 3)\n",
        "\n",
        "            # Paste shadow \n",
        "            shadow_x = x + 3\n",
        "            shadow_y = y + 3\n",
        "            canvas.paste(shadow, (shadow_x, shadow_y), shadow)\n",
        "\n",
        "            # Paste item\n",
        "            canvas.paste(item_img, (x, y), item_img)\n",
        "\n",
        "            print(f\"  {category} placed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error with {category}: {e}\")\n",
        "\n",
        "    return canvas.convert('RGB')\n",
        "\n",
        "\n",
        "def generate_hybrid_flatlay(result):\n",
        "    print(\" HYBRID AI FLATLAY GENERATOR\")\n",
        "\n",
        "    # Extract outfit images\n",
        "    outfit_images = extract_outfit_images(result)\n",
        "\n",
        "    if not outfit_images:\n",
        "        print(\"No outfit images found\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nUsing {len(outfit_images)} items\\n\")\n",
        "\n",
        "    # Generate AI background with accessories\n",
        "    background = generate_background()\n",
        "\n",
        "    print(\"\\nBackground generated!\")\n",
        "    print(\"\\nAI-Generated Background (very light & simple):\")\n",
        "    display(background)\n",
        "\n",
        "    # Composite real images\n",
        "    final_image = create_hybrid_flatlay(outfit_images, background)\n",
        "\n",
        "    # Save\n",
        "    output_path = \"hybrid_flatlay_real_clothes.png\"\n",
        "    final_image.save(output_path)\n",
        "\n",
        "    print(\"GENERATION COMPLETE!\")\n",
        "    print(f\"\\nSaved as: {output_path}\\n\")\n",
        "\n",
        "    print(\"Final Result:\")\n",
        "    display(final_image)\n",
        "\n",
        "    white_bg = Image.new('RGB', final_image.size, (255, 255, 255))\n",
        "    final_no_bg = create_hybrid_flatlay(outfit_images, white_bg)\n",
        "    final_no_bg.save(\"hybrid_flatlay_white_bg.png\")\n",
        "\n",
        "    print(\"\\nsaved version with white background: hybrid_flatlay_white_bg.png\")\n",
        "\n",
        "    return final_image\n",
        "\n",
        "\n",
        "result = show_outfit_recommendation(include_accessories=True)\n",
        "generated = generate_hybrid_flatlay(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
